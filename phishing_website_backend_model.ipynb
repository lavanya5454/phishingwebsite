{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiFEvenHeb4GCPN13Djtyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanya5454/phishingwebsite/blob/main/phishing_website_backend_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yayw_P8Mabb2",
        "outputId": "75e23996-cfe1-4e87-abb8-8605afb5b0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install lightgbm xgboost wordcloud scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import time\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from lightgbm import LGBMClassifier\n",
        "import xgboost as xgb\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PHS6zhC6cuLe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "e4175cd9-12d4-46ab-87bf-5ba04c230da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MALICIOUS URL DETECTION SYSTEM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n[1/10] Loading dataset...\")\n",
        "df = pd.read_csv('malicious_phish.csv')\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded: {df.shape[0]:,} URLs\")\n",
        "print(f\"\\nClass Distribution:\")\n",
        "for cls, count in df['type'].value_counts().items():\n",
        "    print(f\"  {cls:12s}: {count:6,} ({count/len(df)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "Ri3J3KXcaosS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "f6Ng2oJAPjcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "AsAiI_NOPP5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n[2/10] Setting up feature extraction functions...\")\n",
        "\n",
        "def count_special_chars(url):\n",
        "    \"\"\"Count special characters in URL\"\"\"\n",
        "    special = re.findall(r'[^a-zA-Z0-9]', str(url))\n",
        "    return len(special)\n",
        "\n",
        "def calculate_entropy(url):\n",
        "    \"\"\"Calculate Shannon entropy of URL\"\"\"\n",
        "    url = str(url)\n",
        "    if len(url) == 0:\n",
        "        return 0\n",
        "    entropy = 0\n",
        "    for x in range(256):\n",
        "        p_x = float(url.count(chr(x))) / len(url)\n",
        "        if p_x > 0:\n",
        "            entropy += - p_x * np.log2(p_x)\n",
        "    return entropy\n",
        "\n",
        "def having_ip_address(url):\n",
        "    \"\"\"Check if URL contains IP address\"\"\"\n",
        "    match = re.search(\n",
        "        r'(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'\n",
        "        r'([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'\n",
        "        r'((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)|'\n",
        "        r'(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)\n",
        "    return 1 if match else 0\n",
        "\n",
        "def has_suspicious_tld(url):\n",
        "    \"\"\"Check for suspicious top-level domains\"\"\"\n",
        "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz', '.top', '.work', '.click']\n",
        "    return 1 if any(tld in str(url).lower() for tld in suspicious_tlds) else 0\n",
        "\n",
        "def has_shortening_service(url):\n",
        "    \"\"\"Check if URL uses shortening service\"\"\"\n",
        "    shortening_services = ['bit.ly', 'goo.gl', 'tinyurl', 'ow.ly', 't.co', 'buff.ly']\n",
        "    return 1 if any(service in str(url).lower() for service in shortening_services) else 0\n",
        "\n",
        "def preprocess_url(url):\n",
        "    \"\"\"Preprocess URL by removing protocol and www\"\"\"\n",
        "    url = str(url).lower()\n",
        "    url = re.sub(r'https?://', '', url)\n",
        "    url = re.sub(r'^www\\.', '', url)\n",
        "    return url\n",
        "\n",
        "def extract_domain(url):\n",
        "    \"\"\"Extract main domain from URL\"\"\"\n",
        "    url = str(url).lower()\n",
        "    url = re.sub(r'https?://', '', url)\n",
        "    url = re.sub(r'^www\\.', '', url)\n",
        "    domain = url.split('/')[0]\n",
        "    parts = domain.split('.')\n",
        "    if len(parts) >= 2:\n",
        "        return '.'.join(parts[-2:])\n",
        "    return domain\n",
        "\n",
        "print(\"‚úÖ Feature extraction functions ready\")"
      ],
      "metadata": {
        "id": "k9dj-Vogax7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "print(\"\\n[3/10] Engineering features...\")\n",
        "\n",
        "# Manual features\n",
        "df['url_length'] = df['url'].apply(lambda x: len(str(x)))\n",
        "df['num_dots'] = df['url'].apply(lambda x: str(x).count('.'))\n",
        "df['num_hyphens'] = df['url'].apply(lambda x: str(x).count('-'))\n",
        "df['num_underscores'] = df['url'].apply(lambda x: str(x).count('_'))\n",
        "df['num_slashes'] = df['url'].apply(lambda x: str(x).count('/'))\n",
        "df['num_questions'] = df['url'].apply(lambda x: str(x).count('?'))\n",
        "df['num_equals'] = df['url'].apply(lambda x: str(x).count('='))\n",
        "df['num_at'] = df['url'].apply(lambda x: str(x).count('@'))\n",
        "df['num_ampersands'] = df['url'].apply(lambda x: str(x).count('&'))\n",
        "df['num_digits'] = df['url'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "df['digit_ratio'] = df['num_digits'] / df['url_length']\n",
        "df['num_special_chars'] = df['url'].apply(count_special_chars)\n",
        "df['entropy'] = df['url'].apply(calculate_entropy)\n",
        "df['use_of_ip'] = df['url'].apply(having_ip_address)\n",
        "df['is_https'] = df['url'].apply(lambda x: 1 if 'https' in str(x).lower() else 0)\n",
        "df['suspicious_tld'] = df['url'].apply(has_suspicious_tld)\n",
        "df['has_shortening'] = df['url'].apply(has_shortening_service)\n",
        "\n",
        "# Preprocessed URL for TF-IDF\n",
        "df['url_preprocessed'] = df['url'].apply(preprocess_url)\n",
        "\n",
        "feature_cols = ['url_length', 'num_dots', 'num_hyphens', 'num_underscores',\n",
        "                'num_slashes', 'num_questions', 'num_equals', 'num_at',\n",
        "                'num_ampersands', 'num_digits', 'digit_ratio', 'num_special_chars',\n",
        "                'entropy', 'use_of_ip', 'is_https', 'suspicious_tld', 'has_shortening']\n",
        "\n",
        "print(f\"‚úÖ Created {len(feature_cols)} manual features\")\n"
      ],
      "metadata": {
        "id": "TPeSl0Nka1dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n[4/10] Creating TF-IDF features...\")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2, 5),\n",
        "    max_features=5000,\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df['url_preprocessed'])\n",
        "print(f\"‚úÖ TF-IDF features: {tfidf_features.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "SD028a4Ja4er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n[5/10] Combining all features...\")\n",
        "\n",
        "manual_features = csr_matrix(df[feature_cols].values)\n",
        "all_features = hstack([manual_features, tfidf_features])\n",
        "\n",
        "print(f\"‚úÖ Total features: {all_features.shape[1]} (Manual: {len(feature_cols)} + TF-IDF: {tfidf_features.shape[1]})\")\n"
      ],
      "metadata": {
        "id": "t6BSVjMUa5b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n[6/10] Encoding labels and splitting data...\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['type'])\n",
        "\n",
        "print(\"Label Mapping:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"  {i} -> {label}\")\n",
        "\n",
        "# Use sample for faster training (adjust sample_size for full dataset)\n",
        "sample_size = min(100000, len(y_encoded))\n",
        "sample_indices = np.random.choice(len(y_encoded), size=sample_size, replace=False)\n",
        "\n",
        "X_sample = all_features[sample_indices]\n",
        "y_sample = y_encoded[sample_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_sample, y_sample, test_size=0.2, random_state=42, stratify=y_sample\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Training samples: {X_train.shape[0]:,}\")\n",
        "print(f\"‚úÖ Testing samples: {X_test.shape[0]:,}\")\n"
      ],
      "metadata": {
        "id": "ZD92vyGNa9ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n[7/10] Training LightGBM classifier...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model = LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.1,\n",
        "    num_leaves=31,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"‚úÖ Model trained in {training_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "yKJ2IED6bAyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "HYJWbnCXsPyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n[8/10] Evaluating model performance...\")\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"MODEL PERFORMANCE\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\n‚úÖ Overall Accuracy: {accuracy*100:.2f}%\\n\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix - Malicious URL Detection', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UK7zo2OFbEzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n[9/10] Setting up domain whitelist and rules...\")\n",
        "\n",
        "KNOWN_LEGITIMATE_DOMAINS = {\n",
        "    # Tech & Social Media\n",
        "    'google.com', 'youtube.com', 'facebook.com', 'twitter.com', 'instagram.com',\n",
        "    'linkedin.com', 'github.com', 'stackoverflow.com', 'reddit.com', 'pinterest.com',\n",
        "    'tiktok.com', 'snapchat.com', 'discord.com', 'telegram.org', 'whatsapp.com',\n",
        "\n",
        "    # E-commerce\n",
        "    'amazon.com', 'ebay.com', 'alibaba.com', 'walmart.com', 'target.com',\n",
        "    'etsy.com', 'shopify.com', 'bestbuy.com',\n",
        "\n",
        "    # Finance\n",
        "    'paypal.com', 'stripe.com', 'chase.com', 'bankofamerica.com', 'wellsfargo.com',\n",
        "    'citibank.com', 'capitalone.com', 'americanexpress.com',\n",
        "\n",
        "    # Streaming\n",
        "    'netflix.com', 'hulu.com', 'disneyplus.com', 'spotify.com', 'soundcloud.com',\n",
        "    'twitch.tv', 'vimeo.com', 'imdb.com',\n",
        "\n",
        "    # News\n",
        "    'cnn.com', 'bbc.com', 'nytimes.com', 'theguardian.com', 'reuters.com',\n",
        "    'bloomberg.com', 'forbes.com', 'techcrunch.com', 'wired.com', 'theverge.com',\n",
        "\n",
        "    # Education\n",
        "    'wikipedia.org', 'coursera.org', 'udemy.com', 'khanacademy.org', 'edx.org',\n",
        "\n",
        "    # Tech Companies\n",
        "    'microsoft.com', 'apple.com', 'ibm.com', 'oracle.com', 'adobe.com',\n",
        "    'salesforce.com', 'zoom.us', 'slack.com', 'dropbox.com',\n",
        "\n",
        "    # Search Engines\n",
        "    'bing.com', 'yahoo.com', 'duckduckgo.com', 'brave.com', 'mozilla.org',\n",
        "}\n",
        "\n",
        "PHISHING_KEYWORDS = [\n",
        "    'verify', 'confirm', 'update', 'secure', 'account', 'login', 'signin',\n",
        "    'banking', 'suspended', 'locked', 'unusual', 'activity', 'alert',\n",
        "]\n",
        "\n",
        "SUSPICIOUS_TLDS = ['.tk', '.ml', '.ga', '.cf', '.gq']\n",
        "\n",
        "print(f\"‚úÖ Whitelist contains {len(KNOWN_LEGITIMATE_DOMAINS)} legitimate domains\")\n"
      ],
      "metadata": {
        "id": "qiQUIgS6bLNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analyze_url_security(url):\n",
        "    \"\"\"\n",
        "    Advanced security analysis with multiple checks\n",
        "    \"\"\"\n",
        "    url_lower = str(url).lower()\n",
        "    domain = extract_domain(url)\n",
        "\n",
        "    # Rule 1: Whitelisted domain\n",
        "    if domain in KNOWN_LEGITIMATE_DOMAINS:\n",
        "        return {\n",
        "            'risk_level': 'SAFE',\n",
        "            'reason': 'Whitelisted legitimate domain',\n",
        "            'confidence': 0.99\n",
        "        }\n",
        "\n",
        "    # Rule 2: Suspicious TLD + phishing keywords\n",
        "    if any(tld in url_lower for tld in SUSPICIOUS_TLDS):\n",
        "        phishing_count = sum(1 for kw in PHISHING_KEYWORDS if kw in url_lower)\n",
        "        if phishing_count >= 2:\n",
        "            return {\n",
        "                'risk_level': 'HIGH_RISK',\n",
        "                'reason': f'Suspicious TLD with phishing keywords',\n",
        "                'confidence': 0.85\n",
        "            }\n",
        "\n",
        "    # Rule 3: Typosquatting detection\n",
        "    for legit_domain in KNOWN_LEGITIMATE_DOMAINS:\n",
        "        legit_name = legit_domain.split('.')[0]\n",
        "        if legit_name in domain and domain != legit_domain:\n",
        "            if not (legit_name == 'youtube' and domain == 'youtu.be'):\n",
        "                return {\n",
        "                    'risk_level': 'HIGH_RISK',\n",
        "                    'reason': f'Possible typosquatting of {legit_domain}',\n",
        "                    'confidence': 0.80\n",
        "                }\n",
        "\n",
        "    # Rule 4: IP address\n",
        "    if having_ip_address(url):\n",
        "        return {\n",
        "            'risk_level': 'MEDIUM_RISK',\n",
        "            'reason': 'Uses IP address instead of domain',\n",
        "            'confidence': 0.75\n",
        "        }\n",
        "\n",
        "    return None\n",
        "\n",
        "def predict_url(url):\n",
        "    \"\"\"\n",
        "    HYBRID URL SAFETY PREDICTION\n",
        "    Combines rule-based checks with ML model\n",
        "\n",
        "    Args:\n",
        "        url (str): URL to check\n",
        "\n",
        "    Returns:\n",
        "        tuple: (prediction, probabilities, reason)\n",
        "    \"\"\"\n",
        "    # Step 1: Rule-based analysis\n",
        "    analysis = analyze_url_security(url)\n",
        "\n",
        "    if analysis:\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "        probs = np.zeros(num_classes)\n",
        "\n",
        "        if analysis['risk_level'] == 'SAFE':\n",
        "            benign_idx = list(label_encoder.classes_).index('benign')\n",
        "            probs[benign_idx] = analysis['confidence']\n",
        "            for i in range(num_classes):\n",
        "                if i != benign_idx:\n",
        "                    probs[i] = (1 - analysis['confidence']) / (num_classes - 1)\n",
        "            return 'benign', probs, analysis['reason']\n",
        "\n",
        "        elif analysis['risk_level'] == 'HIGH_RISK':\n",
        "            phishing_idx = list(label_encoder.classes_).index('phishing')\n",
        "            probs[phishing_idx] = analysis['confidence']\n",
        "            for i in range(num_classes):\n",
        "                if i != phishing_idx:\n",
        "                    probs[i] = (1 - analysis['confidence']) / (num_classes - 1)\n",
        "            return 'phishing', probs, analysis['reason']\n",
        "\n",
        "        elif analysis['risk_level'] == 'MEDIUM_RISK':\n",
        "            malware_idx = list(label_encoder.classes_).index('malware')\n",
        "            probs[malware_idx] = analysis['confidence']\n",
        "            for i in range(num_classes):\n",
        "                if i != malware_idx:\n",
        "                    probs[i] = (1 - analysis['confidence']) / (num_classes - 1)\n",
        "            return 'malware', probs, analysis['reason']\n",
        "\n",
        "    # Step 2: ML Model prediction\n",
        "    url_str = str(url)\n",
        "    url_len = len(url_str)\n",
        "    num_digits = sum(c.isdigit() for c in url_str)\n",
        "\n",
        "    features_dict = {\n",
        "        'url_length': url_len,\n",
        "        'num_dots': url_str.count('.'),\n",
        "        'num_hyphens': url_str.count('-'),\n",
        "        'num_underscores': url_str.count('_'),\n",
        "        'num_slashes': url_str.count('/'),\n",
        "        'num_questions': url_str.count('?'),\n",
        "        'num_equals': url_str.count('='),\n",
        "        'num_at': url_str.count('@'),\n",
        "        'num_ampersands': url_str.count('&'),\n",
        "        'num_digits': num_digits,\n",
        "        'digit_ratio': num_digits / max(url_len, 1),\n",
        "        'num_special_chars': count_special_chars(url),\n",
        "        'entropy': calculate_entropy(url),\n",
        "        'use_of_ip': having_ip_address(url),\n",
        "        'is_https': 1 if 'https' in url_str.lower() else 0,\n",
        "        'suspicious_tld': has_suspicious_tld(url),\n",
        "        'has_shortening': has_shortening_service(url)\n",
        "    }\n",
        "\n",
        "    manual_values = [features_dict[col] for col in feature_cols]\n",
        "    manual_sparse = csr_matrix([manual_values])\n",
        "\n",
        "    url_preprocessed = preprocess_url(url)\n",
        "    tfidf_sparse = tfidf_vectorizer.transform([url_preprocessed])\n",
        "\n",
        "    X_single = hstack([manual_sparse, tfidf_sparse])\n",
        "\n",
        "    pred_encoded = model.predict(X_single)[0]\n",
        "    pred_label = label_encoder.inverse_transform([pred_encoded])[0]\n",
        "    probs = model.predict_proba(X_single)[0]\n",
        "\n",
        "    return pred_label, probs, 'Machine Learning prediction'\n",
        "\n",
        "def check_url(url):\n",
        "    \"\"\"\n",
        "    User-friendly URL safety checker\n",
        "\n",
        "    Args:\n",
        "        url (str): URL to check\n",
        "\n",
        "    Returns:\n",
        "        dict: Safety analysis results\n",
        "    \"\"\"\n",
        "    pred, probs, reason = predict_url(url)\n",
        "    domain = extract_domain(url)\n",
        "\n",
        "    return {\n",
        "        'url': url,\n",
        "        'domain': domain,\n",
        "        'prediction': pred,\n",
        "        'is_safe': pred == 'benign',\n",
        "        'confidence': float(max(probs)),\n",
        "        'reason': reason,\n",
        "        'whitelisted': domain in KNOWN_LEGITIMATE_DOMAINS,\n",
        "        'probabilities': {\n",
        "            label_encoder.classes_[i]: float(probs[i])\n",
        "            for i in range(len(label_encoder.classes_))\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Hybrid prediction system ready\")\n"
      ],
      "metadata": {
        "id": "u_ybI5bhbO8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n[10/10] Running demonstration tests...\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"DEMONSTRATION - URL SAFETY CHECKER\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "test_urls = [\n",
        "    \"https://web.whatsapp.com/\",\n",
        "    \"http://bet8.pages.dev\",\n",
        "    \"https://www.youtube.com/watch?v=AlBTv_eBPd4\",\n",
        "    \"http://paypal-verify-account.tk\",\n",
        "    \"https://gotoworkb.vip/z7mF79/#/\",\n",
        "    \"\thttps://richardsamuelmd.com/sso/login/\",\n",
        "    \"http://192.168.0.100/admin\"]\n",
        "\n",
        "for url in test_urls:\n",
        "    result = check_url(url)\n",
        "\n",
        "    if result['is_safe']:\n",
        "        status = \"üü¢ SAFE\"\n",
        "        color = '\\033[92m'  # Green\n",
        "    else:\n",
        "        status = \"üî¥ MALICIOUS\"\n",
        "        color = '\\033[91m'  # Red\n",
        "    reset = '\\033[0m'\n",
        "\n",
        "    print(f\"\\n{color}{status}{reset}\")\n",
        "    print(f\"URL: {url}\")\n",
        "    print(f\"Prediction: {result['prediction'].upper()}\")\n",
        "    print(f\"Confidence: {result['confidence']*100:.1f}%\")\n",
        "    print(f\"Reason: {result['reason']}\")\n",
        "    if result['whitelisted']:\n",
        "        print(f\"‚úì Whitelisted domain\")"
      ],
      "metadata": {
        "id": "x4BnummzbfHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"SAVING MODEL\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "model_package = {\n",
        "    'model': model,\n",
        "    'tfidf_vectorizer': tfidf_vectorizer,\n",
        "    'label_encoder': label_encoder,\n",
        "    'feature_cols': feature_cols,\n",
        "    'whitelist': KNOWN_LEGITIMATE_DOMAINS,\n",
        "    'version': '1.0',\n",
        "    'training_accuracy': accuracy,\n",
        "    'training_date': time.strftime('%Y-%m-%d')\n",
        "}\n",
        "\n",
        "with open('malicious_url_detector.pkl', 'wb') as f:\n",
        "    pickle.dump(model_package, f)\n",
        "\n",
        "print(\"\\n‚úÖ Model saved to 'malicious_url_detector.pkl'\")"
      ],
      "metadata": {
        "id": "XHNV0K89bgVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIOeX4co-Whj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_mmjq-5V-YEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def interactive_checker():\n",
        "    \"\"\"\n",
        "    Interactive URL checker - keep testing URLs\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"INTERACTIVE URL SAFETY CHECKER\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"Enter URLs to check (type 'quit' to exit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        url = input(\"üîó Enter URL: \").strip()\n",
        "\n",
        "        if url.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"\\nüëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not url:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = check_url(url)\n",
        "\n",
        "            print(\"\\n\" + \"-\"*70)\n",
        "            if result['is_safe']:\n",
        "                print(\"‚úÖ SAFE - This URL appears to be legitimate\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  WARNING - This URL is classified as: {result['prediction'].upper()}\")\n",
        "\n",
        "            print(f\"Confidence: {result['confidence']*100:.1f}%\")\n",
        "            print(f\"Reason: {result['reason']}\")\n",
        "\n",
        "            print(\"\\nDetailed Probabilities:\")\n",
        "            for cls, prob in sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True):\n",
        "                bar = '‚ñà' * int(prob * 50)\n",
        "                print(f\"  {cls:12s}: {prob*100:5.1f}% {bar}\")\n",
        "            print(\"-\"*70 + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\\n\")"
      ],
      "metadata": {
        "id": "XZ6wvSjxbn8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\"\"\n",
        "‚úÖ PROJECT COMPLETED SUCCESSFULLY!\n",
        "\n",
        "üìä Model Statistics:\n",
        "   - Training Samples: {X_train.shape[0]:,}\n",
        "   - Testing Samples: {X_test.shape[0]:,}\n",
        "   - Features: {all_features.shape[1]}\n",
        "   - Accuracy: {accuracy*100:.2f}%\n",
        "   - Training Time: {training_time:.2f} seconds\n",
        "\n",
        "üîß Features:\n",
        "   - 17 Manual engineered features\n",
        "   - 5000 TF-IDF character n-grams\n",
        "   - Hybrid rule-based + ML approach\n",
        "   - Whitelist of {len(KNOWN_LEGITIMATE_DOMAINS)} legitimate domains\n",
        "\n",
        "üéØ Usage:\n",
        "   1. Simple check: result = check_url('https://example.com')\n",
        "   2. Interactive: interactive_checker()\n",
        "   3. Direct predict: prediction, probs, reason = predict_url('url')\n",
        "\n",
        "üìÅ Saved Files:\n",
        "   - malicious_url_detector.pkl (model file)\n",
        "\n",
        "üöÄ Ready for deployment and demonstration!\n",
        "\"\"\")\n",
        "\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Pvo1aLRYbo5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate ngrok\n",
        "ngrok.set_auth_token(\"358iAUThEHTDr2fTWujUnTCrLOC_2aSVWBLTc2yJJ8A6N2WSt\")\n"
      ],
      "metadata": {
        "id": "oG12x00IkYul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok lightgbm xgboost wordcloud scikit-learn pandas numpy seaborn matplotlib\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ac2QeG2aDo81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0382697-1101-4ad1-c778-8c7777cc7aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.10.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install streamlit pyngrok lightgbm xgboost wordcloud scikit-learn\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MbwMtSGVEWR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# List all tunnels\n",
        "tunnels = ngrok.get_tunnels()\n",
        "for t in tunnels:\n",
        "    print(t.public_url)\n",
        "    # Kill each tunnel\n",
        "    ngrok.disconnect(t.public_url)\n"
      ],
      "metadata": {
        "id": "lo2U0UgsysHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Disconnect all active tunnels\n",
        "for t in ngrok.get_tunnels():\n",
        "    print(f\"Disconnecting tunnel: {t.public_url}\")\n",
        "    ngrok.disconnect(t.public_url)\n"
      ],
      "metadata": {
        "id": "o8F39RCJyws3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit URL:\", public_url.public_url)\n"
      ],
      "metadata": {
        "id": "-2YT3VcVy7Ar",
        "outputId": "ca7e0e6e-1322-4d66-fcbe-3d75222ec6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-11-08T14:08:04+0000 lvl=warn msg=\"failed to start tunnel\" pg=/api/tunnels id=2c516f643e1f9e9a err=\"failed to start tunnel: The endpoint 'https://avulsed-unendorsed-alpha.ngrok-free.dev' is already online. Either\\n1. stop your existing endpoint first, or\\n2. start both endpoints with `--pooling-enabled` to load balance between them.\\r\\n\\r\\nERR_NGROK_334\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokHTTPError",
          "evalue": "ngrok client exception, API returned 502: {\"error_code\":103,\"status_code\":502,\"msg\":\"failed to start tunnel\",\"details\":{\"err\":\"failed to start tunnel: The endpoint 'https://avulsed-unendorsed-alpha.ngrok-free.dev' is already online. Either\\n1. stop your existing endpoint first, or\\n2. start both endpoints with `--pooling-enabled` to load balance between them.\\r\\n\\r\\nERR_NGROK_334\\r\\n\"}}\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout, auth)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    631\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4267711612.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Streamlit URL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     tunnel = NgrokTunnel(api_request(f\"{api_url}/api/tunnels\", method=\"POST\", data=options,\n\u001b[0m\u001b[1;32m    392\u001b[0m                                      timeout=pyngrok_config.request_timeout),\n\u001b[1;32m    393\u001b[0m                          pyngrok_config, api_url)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout, auth)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response {status_code}: {response_data.strip()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         raise PyngrokNgrokHTTPError(f\"ngrok client exception, API returned {status_code}: {response_data}\",\n\u001b[0m\u001b[1;32m    651\u001b[0m                                     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                                     status_code, e.reason, e.headers, response_data)\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m: ngrok client exception, API returned 502: {\"error_code\":103,\"status_code\":502,\"msg\":\"failed to start tunnel\",\"details\":{\"err\":\"failed to start tunnel: The endpoint 'https://avulsed-unendorsed-alpha.ngrok-free.dev' is already online. Either\\n1. stop your existing endpoint first, or\\n2. start both endpoints with `--pooling-enabled` to load balance between them.\\r\\n\\r\\nERR_NGROK_334\\r\\n\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate ngrok\n",
        "ngrok.set_auth_token(\"358iAUThEHTDr2fTWujUnTCrLOC_2aSVWBLTc2yJJ8A6N2WSt\")\n"
      ],
      "metadata": {
        "id": "QClDFgjtEXEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 ‚Äî install dependencies (run once)\n",
        "!pip install -q streamlit pyngrok lightgbm xgboost wordcloud scikit-learn\n",
        "\n",
        "# small helper for Colab to show the streaming logs\n",
        "import os\n",
        "os.environ[\"STREAMLIT_SERVER_HEADLESS\"] = \"true\"\n",
        "print(\"‚úÖ Installed dependencies.\")\n"
      ],
      "metadata": {
        "id": "2uvv6JxnLITm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a223d4-3bc9-4816-beb9-51eb1142fa60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Installed dependencies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 ‚Äî write the Streamlit app file\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import numpy as np\n",
        "import re\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import os\n",
        "\n",
        "st.set_page_config(page_title=\"Malicious URL Detector\", layout=\"wide\")\n",
        "\n",
        "# === Load model helper ===\n",
        "@st.cache_data(show_spinner=False)\n",
        "def load_model(path=\"malicious_url_detector.pkl\"):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, \"rb\") as f:\n",
        "        pkg = pickle.load(f)\n",
        "    return pkg\n",
        "\n",
        "pkg = load_model()\n",
        "if pkg is None:\n",
        "    st.error(\"Model file 'malicious_url_detector.pkl' not found in working directory. Upload it to Colab or run training cells first.\")\n",
        "    st.stop()\n",
        "\n",
        "model = pkg['model']\n",
        "tfidf_vectorizer = pkg['tfidf_vectorizer']\n",
        "label_encoder = pkg['label_encoder']\n",
        "feature_cols = pkg['feature_cols']\n",
        "KNOWN_LEGITIMATE_DOMAINS = set(pkg.get('whitelist', []))\n",
        "\n",
        "# === Helper functions (same as notebook) ===\n",
        "def count_special_chars(url):\n",
        "    return len(re.findall(r'[^a-zA-Z0-9]', str(url)))\n",
        "\n",
        "def calculate_entropy(url):\n",
        "    url = str(url)\n",
        "    if len(url) == 0:\n",
        "        return 0.0\n",
        "    entropy = 0.0\n",
        "    for x in range(256):\n",
        "        p_x = float(url.count(chr(x))) / len(url)\n",
        "        if p_x > 0:\n",
        "            entropy += - p_x * np.log2(p_x)\n",
        "    return entropy\n",
        "\n",
        "def having_ip_address(url):\n",
        "    return 1 if re.search(\n",
        "        r'(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5]))',\n",
        "        str(url)) else 0\n",
        "\n",
        "def has_suspicious_tld(url):\n",
        "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz', '.top', '.work', '.click']\n",
        "    return 1 if any(tld in str(url).lower() for tld in suspicious_tlds) else 0\n",
        "\n",
        "def has_shortening_service(url):\n",
        "    shortening_services = ['bit.ly', 'goo.gl', 'tinyurl', 'ow.ly', 't.co', 'buff.ly']\n",
        "    return 1 if any(service in str(url).lower() for service in shortening_services) else 0\n",
        "\n",
        "def preprocess_url(url):\n",
        "    url = str(url).lower()\n",
        "    url = re.sub(r'https?://', '', url)\n",
        "    url = re.sub(r'^www\\.', '', url)\n",
        "    return url\n",
        "\n",
        "def extract_domain(url):\n",
        "    url = str(url).lower()\n",
        "    url = re.sub(r'https?://', '', url)\n",
        "    url = re.sub(r'^www\\.', '', url)\n",
        "    domain = url.split('/')[0]\n",
        "    parts = domain.split('.')\n",
        "    return '.'.join(parts[-2:]) if len(parts) >= 2 else domain\n",
        "\n",
        "PHISHING_KEYWORDS = [\n",
        "    'verify', 'confirm', 'update', 'secure', 'account', 'login', 'signin',\n",
        "    'banking', 'suspended', 'locked', 'unusual', 'activity', 'alert',\n",
        "]\n",
        "SUSPICIOUS_TLDS = ['.tk', '.ml', '.ga', '.cf', '.gq']\n",
        "\n",
        "def analyze_url_security(url):\n",
        "    url_lower = str(url).lower()\n",
        "    domain = extract_domain(url)\n",
        "\n",
        "    # Rule 1: Whitelisted domain\n",
        "    if domain in KNOWN_LEGITIMATE_DOMAINS:\n",
        "        return ('SAFE', 'Whitelisted legitimate domain', 0.99)\n",
        "\n",
        "    # Rule 2: Suspicious TLD + phishing keywords\n",
        "    if any(tld in url_lower for tld in SUSPICIOUS_TLDS):\n",
        "        phishing_count = sum(1 for kw in PHISHING_KEYWORDS if kw in url_lower)\n",
        "        if phishing_count >= 2:\n",
        "            return ('HIGH_RISK', 'Suspicious TLD with phishing keywords', 0.85)\n",
        "\n",
        "    # Rule 3: Typosquatting detection (simple)\n",
        "    for legit_domain in KNOWN_LEGITIMATE_DOMAINS:\n",
        "        legit_name = legit_domain.split('.')[0]\n",
        "        if legit_name in domain and domain != legit_domain:\n",
        "            if not (legit_name == 'youtube' and domain == 'youtu.be'):\n",
        "                return ('HIGH_RISK', f'Possible typosquatting of {legit_domain}', 0.80)\n",
        "\n",
        "    # Rule 4: IP address\n",
        "    if having_ip_address(url):\n",
        "        return ('MEDIUM_RISK', 'Uses IP address instead of domain', 0.75)\n",
        "\n",
        "    return None\n",
        "\n",
        "def predict_with_model(url):\n",
        "    url_str = str(url)\n",
        "    url_len = len(url_str)\n",
        "    num_digits = sum(c.isdigit() for c in url_str)\n",
        "\n",
        "    features_dict = {\n",
        "        'url_length': url_len,\n",
        "        'num_dots': url_str.count('.'),\n",
        "        'num_hyphens': url_str.count('-'),\n",
        "        'num_underscores': url_str.count('_'),\n",
        "        'num_slashes': url_str.count('/'),\n",
        "        'num_questions': url_str.count('?'),\n",
        "        'num_equals': url_str.count('='),\n",
        "        'num_at': url_str.count('@'),\n",
        "        'num_ampersands': url_str.count('&'),\n",
        "        'num_digits': num_digits,\n",
        "        'digit_ratio': num_digits / max(url_len, 1),\n",
        "        'num_special_chars': count_special_chars(url),\n",
        "        'entropy': calculate_entropy(url),\n",
        "        'use_of_ip': having_ip_address(url),\n",
        "        'is_https': 1 if 'https' in url_str.lower() else 0,\n",
        "        'suspicious_tld': has_suspicious_tld(url),\n",
        "        'has_shortening': has_shortening_service(url)\n",
        "    }\n",
        "\n",
        "    manual_values = [features_dict[c] for c in feature_cols]\n",
        "    manual_sparse = csr_matrix([manual_values])\n",
        "    tfidf_sparse = tfidf_vectorizer.transform([preprocess_url(url)])\n",
        "    X_single = hstack([manual_sparse, tfidf_sparse])\n",
        "\n",
        "    pred_encoded = model.predict(X_single)[0]\n",
        "    pred_label = label_encoder.inverse_transform([pred_encoded])[0]\n",
        "    probs = model.predict_proba(X_single)[0]\n",
        "    probs_dict = {label_encoder.classes_[i]: float(probs[i]) for i in range(len(probs))}\n",
        "    return pred_label, float(np.max(probs)), probs_dict\n",
        "\n",
        "# === UI Layout ===\n",
        "st.title(\"üîç Malicious URL Detection ‚Äî Hybrid (rules + ML)\")\n",
        "col1, col2 = st.columns([3,1])\n",
        "\n",
        "with col1:\n",
        "    url_input = st.text_input(\"Enter URL to analyze\", value=\"https://www.example.com\")\n",
        "    if st.button(\"Analyze URL\"):\n",
        "        if not url_input.strip():\n",
        "            st.warning(\"Please enter a URL.\")\n",
        "        else:\n",
        "            analysis = analyze_url_security(url_input)\n",
        "            if analysis:\n",
        "                risk, reason, conf = analysis\n",
        "                if risk == 'SAFE':\n",
        "                    displayed_label = 'benign'\n",
        "                    st.success(f\"üü¢ SAFE ‚Äî {reason} (confidence {conf*100:.1f}%)\")\n",
        "                elif risk == 'HIGH_RISK':\n",
        "                    displayed_label = 'phishing'\n",
        "                    st.error(f\"üî¥ HIGH RISK ‚Äî {reason} (confidence {conf*100:.1f}%)\")\n",
        "                else:\n",
        "                    displayed_label = 'malware'\n",
        "                    st.warning(f\"‚ö†Ô∏è MEDIUM RISK ‚Äî {reason} (confidence {conf*100:.1f}%)\")\n",
        "\n",
        "                # Build estimated probs vector (simple)\n",
        "                classes = list(label_encoder.classes_)\n",
        "                probs = {c: 0.0 for c in classes}\n",
        "                probs[displayed_label] = conf\n",
        "                # distribute remaining prob evenly (for display)\n",
        "                remaining = (1 - conf)\n",
        "                others = [c for c in classes if c != displayed_label]\n",
        "                for o in others:\n",
        "                    probs[o] = remaining / max(1, len(others))\n",
        "\n",
        "                st.write(\"**Probabilities (estimated):**\")\n",
        "                for cls, p in sorted(probs.items(), key=lambda x: x[1], reverse=True):\n",
        "                    bar = '‚ñà' * int(p * 40)\n",
        "                    st.write(f\"- {cls:12s}: {p*100:5.1f}% {bar}\")\n",
        "\n",
        "                st.write(\"---\")\n",
        "                st.write(\"**Reason:**\", reason)\n",
        "            else:\n",
        "                pred_label, conf, probs_dict = predict_with_model(url_input)\n",
        "                if pred_label == 'benign':\n",
        "                    st.success(f\"üü¢ SAFE ‚Äî {pred_label} (confidence {conf*100:.1f}%)\")\n",
        "                else:\n",
        "                    st.error(f\"üî¥ {pred_label.upper()} ‚Äî (confidence {conf*100:.1f}%)\")\n",
        "\n",
        "                st.write(\"**Probabilities:**\")\n",
        "                for cls, p in sorted(probs_dict.items(), key=lambda x: x[1], reverse=True):\n",
        "                    bar = '‚ñà' * int(p * 40)\n",
        "                    st.write(f\"- {cls:12s}: {p*100:5.1f}% {bar}\")\n",
        "\n",
        "            st.write(\"---\")\n",
        "            st.write(\"**Domain:**\", extract_domain(url_input))\n",
        "            st.write(\"**Whitelisted:**\", extract_domain(url_input) in KNOWN_LEGITIMATE_DOMAINS)\n",
        "\n",
        "with col2:\n",
        "    st.markdown(\"### Quick examples\")\n",
        "    if st.button(\"Phishing example 1\"):\n",
        "        st.experimental_set_query_params()  # noop\n",
        "        st.session_state[\"url_example\"] = \"http://paypal-verify-account.tk\"\n",
        "        st.warning(\"Example: http://paypal-verify-account.tk ‚Äî copy into main input and click Analyze\")\n",
        "    if st.button(\"Phishing example 2\"):\n",
        "        st.session_state[\"url_example\"] = \"http://google-secure-login.tk\"\n",
        "        st.warning(\"Example: http://google-secure-login.tk ‚Äî copy into main input and click Analyze\")\n",
        "    if st.button(\"Shortened example\"):\n",
        "        st.session_state[\"url_example\"] = \"https://bit.ly/confirm-account\"\n",
        "        st.warning(\"Example: https://bit.ly/confirm-account ‚Äî copy into main input and click Analyze\")\n",
        "    if st.button(\"Benign example (Google)\"):\n",
        "        st.session_state[\"url_example\"] = \"https://www.google.com\"\n",
        "        st.success(\"Example: https://www.google.com ‚Äî copy into main input and click Analyze\")\n",
        "\n",
        "    st.write(\"---\")\n",
        "    st.markdown(\"**Notes**\")\n",
        "    st.write(\"- Examples are synthetic for testing only.\")\n",
        "    st.write(\"- Do NOT paste real, active malicious URLs with tokens/credentials into public notebooks.\")\n",
        "\n",
        "st.caption(\"Model version: {}\".format(pkg.get('version', 'unknown')))\n"
      ],
      "metadata": {
        "id": "ukfBFfpALm2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 ‚Äî upload model if you haven't already, authenticate ngrok, and run Streamlit\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# If you don't have the model in the session, upload it now\n",
        "if not os.path.exists(\"malicious_url_detector.pkl\"):\n",
        "    print(\"Upload 'malicious_url_detector.pkl' now (choose file).\")\n",
        "    uploaded = files.upload()  # interactively upload\n",
        "    if \"malicious_url_detector.pkl\" not in uploaded:\n",
        "        print(\"Make sure the uploaded filename is exactly 'malicious_url_detector.pkl'.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Uploaded model file.\")\n",
        "\n",
        "# Authenticate ngrok (replace token)\n",
        "from pyngrok import ngrok, conf\n",
        "NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTHTOKEN_HERE\"  # <<--- REPLACE this with your token\n",
        "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTHTOKEN_HERE\":\n",
        "    print(\"‚ö†Ô∏è Please replace NGROK_AUTH_TOKEN with your actual token from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "else:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "print(\"Starting ngrok tunnel on port 8501...\")\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üåê Streamlit URL:\", public_url.public_url)\n",
        "\n",
        "# Launch Streamlit\n",
        "print(\"Launching Streamlit app (this runs in the background). Logs will appear below.\")\n",
        "get_ipython().system_raw('streamlit run app.py --server.port 8501 &')\n",
        "\n",
        "# Optional: show the last lines of the streamlit log for status\n",
        "import time, subprocess, sys\n",
        "time.sleep(2)\n",
        "print(\"You can open the URL above. If it doesn't load immediately, wait a few seconds and refresh.\")\n"
      ],
      "metadata": {
        "id": "7bC9MjhGMHFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9lpl9IGeo4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QO2yr_PrL6pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# frontend 2"
      ],
      "metadata": {
        "id": "_DrSZ_2Pdwdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill all active tunnels\n",
        "ngrok.kill()\n",
        "\n"
      ],
      "metadata": {
        "id": "11Zv3tDZTkxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLUuukdygAYa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}